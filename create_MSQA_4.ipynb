{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"amphora/krx-sample-instructions\", split = \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 역색인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.index = defaultdict(dict)\n",
    "        self.kiwi = Kiwi()\n",
    "        self.document_lengths = {}\n",
    "        self.total_documents = 0\n",
    "        self.average_document_length = 0\n",
    "        self.documents = {}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [token.form for token in self.kiwi.tokenize(text)]\n",
    "\n",
    "    def add_document(self, doc_id, question, answer):\n",
    "        tokens = self.tokenize(question)\n",
    "        self.document_lengths[doc_id] = len(tokens)\n",
    "        self.total_documents += 1\n",
    "        self.documents[doc_id] = {'question': question, 'answer': answer}\n",
    "\n",
    "        for token in set(tokens):\n",
    "            if doc_id not in self.index[token]:\n",
    "                self.index[token][doc_id] = 0\n",
    "            self.index[token][doc_id] += tokens.count(token)\n",
    "\n",
    "        self.average_document_length = sum(self.document_lengths.values()) / self.total_documents\n",
    "\n",
    "    def calculate_bm25_score(self, query_tokens, doc_id):\n",
    "        k1 = 1.5\n",
    "        b = 0.75\n",
    "        score = 0\n",
    "\n",
    "        for token in query_tokens:\n",
    "            if token not in self.index or doc_id not in self.index[token]:\n",
    "                continue\n",
    "\n",
    "            tf = self.index[token][doc_id]\n",
    "            df = len(self.index[token])\n",
    "            idf = math.log((self.total_documents - df + 0.5) / (df + 0.5) + 1)\n",
    "\n",
    "            numerator = tf * (k1 + 1)\n",
    "            denominator = tf + k1 * (1 - b + b * self.document_lengths[doc_id] / self.average_document_length)\n",
    "            score += idf * numerator / denominator\n",
    "\n",
    "        return score\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        query_tokens = self.tokenize(query)\n",
    "        scores = defaultdict(float)\n",
    "\n",
    "        for token in query_tokens:\n",
    "            if token in self.index:\n",
    "                for doc_id in self.index[token]:\n",
    "                    scores[doc_id] += self.calculate_bm25_score(query_tokens, doc_id)\n",
    "\n",
    "        top_k = sorted(scores.items(), key=lambda x: x[1], reverse = True)[:k]\n",
    "        return [(doc_id, score, self.documents[doc_id]) for doc_id, score in top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25951/25951 [01:27<00:00, 296.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# add docs\n",
    "from tqdm import tqdm\n",
    "\n",
    "index = InvertedIndex()\n",
    "for idx, data in enumerate(tqdm(dataset)):\n",
    "    question = data['prompt']\n",
    "    answer = data['response']\n",
    "    index.add_document(idx, question, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비슷한 문제들을 참고해서 만들게 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "dataset = random.sample(list(dataset), k=1000)\n",
    "dataset = dataset[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "openai_key = os.getenv('OPENAI_TEAM_API_KEY')\n",
    "client = openai.OpenAI(api_key=openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are given two pairs of reference questions and reference answers.\n",
    "Your role is a questioner who make a new question.\n",
    "When making your questions, consider the following.\n",
    "1. New Question must require choices such as 'Which is right', 'Which is not right', 'Which is most appropriate', and 'Which is not most appropriate'.\n",
    "2. You have to make 5 choices, 1 answer choice and 4 wrong choices.\n",
    "3. The choices must be generated in association with one of several keywords in the reference question and answer.\n",
    "4. The wrong answer and the right answer are confused, but the wrong answer must be a clear wrong answer.\n",
    "5. The choices does not deviate from the subject of the problem, but it must be different.\n",
    "6. The choices sentence must be similar in length.\n",
    "7. If a person is a financial expert, the person can solve the problem, but if the person is a beginner in financial knowledge, please make the problem with a difficulty that the person cannot solve because it is difficult.\n",
    "8. Please don't create a problem that can be solved by reading other than the problem.\n",
    "please write in Korean and you must write the answer on the last line.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt =\"\"\"\n",
    "### Reference\n",
    "### Question 1: {}\n",
    "### Answer 1: {}\n",
    "\n",
    "### Question 2: {}\n",
    "### Answer 2: {}\n",
    "\n",
    "### New Question : \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:40<1:06:27, 40.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주어진 편미분 방정식이 주어졌을 때, $S_t$와 $F_t$의 관계를 이해하기 위해 어떤 접근 방법이 가장 적절한가? 다음 중 가장 적절한 선택은 무엇인가?\n",
      "\n",
      "1. $S_t$에 대한 확률적 미분 방정식을 다시 쓰고 그 해를 분석한다.\n",
      "2. $dW$의 성질을 무시하고 $F_t$를 단순히 $S_t$로 대체한다.\n",
      "3. 각각의 미분 방정식에서 모든 변수 간의 관계를 동일하게 본다.\n",
      "4. $F_t$의 확률적 동작과 $S_t$ 간의 상관관계를 무시한다.\n",
      "5. $S_t$와 $F_t$의 stochastic 과정에 대한 명확한 법칙을 따르고 그를 바탕으로 비교한다.\n",
      "\n",
      "정답은 5번입니다.\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [1:14:39<00:00, 44.79s/it] \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "new_questions = []\n",
    "for idx, data in enumerate(tqdm(dataset, total=len(dataset))):\n",
    "    question = data['prompt']\n",
    "    answer = data['response']\n",
    "    system_msg = {'role':'system', 'content': system_prompt}\n",
    "\n",
    "    similar_questions = index.search(question, k=2)\n",
    "\n",
    "    user_msg = {'role':'user', 'content': user_prompt.format(\n",
    "        similar_questions[0][2]['question'],\n",
    "        similar_questions[0][2]['answer'],\n",
    "        similar_questions[1][2]['question'],\n",
    "        similar_questions[1][2]['answer'])}\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages = [system_msg, user_msg],\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    new_questions.append(result)\n",
    "    \n",
    "    if idx % 100 == 0:\n",
    "        print(result)\n",
    "        print(\"*\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV 파일로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqa_data = []\n",
    "\n",
    "for raw_question in new_questions:\n",
    "    lines = [line.strip() for line in raw_question.split('\\n') if line.strip() != '']\n",
    "    question = '\\n'.join(lines[:-1])\n",
    "    answer = lines[-1]\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        if str(i) in answer:\n",
    "            answer = i\n",
    "            break\n",
    "\n",
    "    item = {\"question\": question, 'answer': answer}\n",
    "    mcqa_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(mcqa_data)\n",
    "result_df.to_csv('/root/KRX_LLM/data/krx_sample_mcqa_data1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
