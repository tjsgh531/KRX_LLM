{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['term', 'context', 'questions'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/root/KRX_LLM/data/finance_terms_context_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n",
      "['2ì°¨ ì‹œì¥(Secondary Market)', '5ì¼ì„ ', 'ABCP(Asset Backed Commercial Paper)', 'AMA(Auto Management Account)', 'At The Money(ATM, ì•³ ë” ë¨¸ë‹ˆ)', 'BIC', 'BIS(Bank for International Settlements, êµ­ì œê²°ì œì€í–‰)', 'Barclays Global Aggregate', 'Behavioral Finance', 'CAMELì§€ìˆ˜(ì¹´ë©œì§€ìˆ˜)']\n"
     ]
    }
   ],
   "source": [
    "terms = []\n",
    "\n",
    "for item in data:\n",
    "    terms.append(item['term'])\n",
    "\n",
    "print(len(terms))\n",
    "print(terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "hf_token = os.getenv(\"HF_TEAM_TOKEN\")\n",
    "print(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.47.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = 'KR-X-AI/krx-qwen2-7b-instruct-v4_m'\n",
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„ ì–¸\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ë‹¤ìŒ ì¤‘ í™”íì˜ ì‹œê°„ê°€ì¹˜ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€?\n",
      "\n",
      "A. ì›” ë³µë¦¬ì˜ ê²½ìš°, ë§¤ì›” ì ìš©ë˜ëŠ” ì´ììœ¨ì€ ì—°ê°„ ëª…ëª© ì´ììœ¨ì„ 1/12ë¡œ ë‚˜ëˆ„ì–´ ì‚°ì¶œí•œë‹¤.\n",
      "B. íˆ¬ì ì›ê¸ˆ ë° ê¸°íƒ€ ì¡°ê±´ì´ ë™ì¼í•  ê²½ìš°, ë‹¨ë¦¬ ë°©ì‹ë³´ë‹¤ ë³µë¦¬ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìê°€ ë” í¬ë‹¤.\n",
      "C. ì¼ì‹œë¶ˆë¡œ ì§€ê¸‰ë  ê¸ˆì•¡ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ ê°€ì¹˜ë¥¼ ì¼ì • ê¸°ê°„ ë™ì•ˆ í• ì¸ìœ¨ì„ ì ìš©í•´ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤.\n",
      "D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
      "     \"\"\"\n",
      "<|im_start|>assistant: D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ì¤‘ í™”íì˜ ì‹œê°„ê°€ì¹˜ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "A. ì›” ë³µë¦¬ì˜ ê²½ìš°, ë§¤ì›” ì ìš©ë˜ëŠ” ì´ììœ¨ì€ ì—°ê°„ ëª…ëª© ì´ììœ¨ì„ 1/12ë¡œ ë‚˜ëˆ„ì–´ ì‚°ì¶œí•œë‹¤.\n",
    "B. íˆ¬ì ì›ê¸ˆ ë° ê¸°íƒ€ ì¡°ê±´ì´ ë™ì¼í•  ê²½ìš°, ë‹¨ë¦¬ ë°©ì‹ë³´ë‹¤ ë³µë¦¬ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìê°€ ë” í¬ë‹¤.\n",
    "C. ì¼ì‹œë¶ˆë¡œ ì§€ê¸‰ë  ê¸ˆì•¡ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ ê°€ì¹˜ë¥¼ ì¼ì • ê¸°ê°„ ë™ì•ˆ í• ì¸ìœ¨ì„ ì ìš©í•´ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤.\n",
    "D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
    "    \"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ë§¤ë„í—¤ì§€\n",
      "í† í°: ['Ã«Â§Â¤', 'Ã«Ä±Ä¦', 'Ã­Ä¹Â¤', 'Ã¬Â§Ä¢']\n",
      "í† í° ìˆ˜: 4\n"
     ]
    }
   ],
   "source": [
    "def show_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"ì›ë¬¸: {text}\")\n",
    "    print(f\"í† í°: {tokens}\")\n",
    "    print(f\"í† í° ìˆ˜: {len(tokens)}\")\n",
    "\n",
    "test_text = terms[250]\n",
    "show_tokens(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë§¤ë„í—¤ì§€\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "test_text = terms[250]\n",
    "encoded = encode_text(test_text)\n",
    "decoded = tokenizer.decode(encoded[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(152450, 3584)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(terms)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ë§¤ë„í—¤ì§€\n",
      "í† í°: ['ë§¤ë„í—¤ì§€']\n",
      "í† í° ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "def show_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"ì›ë¬¸: {text}\")\n",
    "    print(f\"í† í°: {tokens}\")\n",
    "    print(f\"í† í° ìˆ˜: {len(tokens)}\")\n",
    "\n",
    "test_text = terms[250]\n",
    "show_tokens(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ë‹¤ìŒ ì¤‘ í™”íì˜ ì‹œê°„ê°€ì¹˜ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€?\n",
      "\n",
      "A. ì›” ë³µë¦¬ì˜ ê²½ìš°, ë§¤ì›” ì ìš©ë˜ëŠ” ì´ììœ¨ì€ ì—°ê°„ ëª…ëª© ì´ììœ¨ì„ 1/12ë¡œ ë‚˜ëˆ„ì–´ ì‚°ì¶œí•œë‹¤.\n",
      "B. íˆ¬ì ì›ê¸ˆ ë° ê¸°íƒ€ ì¡°ê±´ì´ ë™ì¼í•  ê²½ìš°, ë‹¨ë¦¬ ë°©ì‹ë³´ë‹¤ ë³µë¦¬ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìê°€ ë” í¬ë‹¤.\n",
      "C. ì¼ì‹œë¶ˆë¡œ ì§€ê¸‰ë  ê¸ˆì•¡ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ ê°€ì¹˜ë¥¼ ì¼ì • ê¸°ê°„ ë™ì•ˆ í• ì¸ìœ¨ì„ ì ìš©í•´ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤.\n",
      "D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
      "     \"\"\"\n",
      "<|im_start|>assistant: D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤. ì´ ì£¼ì¥ì€ ì˜³ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "ë³µë¦¬ ì´ì ê³„ì‚°ì‹ì€ P * (1 + r )^n ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìœ¼ë©°, ì´ ì‹ì—ì„œ PëŠ” ì›ê¸ˆ, rì€ ì´ììœ¨, nì€ ê¸°ê°„ì…ë‹ˆë‹¤. ì´ ë¬¸ì œì˜ ê²½ìš°, ì›ê¸ˆ(P)ì€ 1,000,000ì›, ì´ììœ¨(r)ì€ 5%(ë˜ëŠ” 0.05), ê¸°ê°„(n)ì€ 2ë…„ì…ë‹ˆë‹¤.\n",
      "\n",
      "ë”°ë¼ì„œ ë§Œê¸° ì‹œì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:\n",
      "\n",
      "1,000,000ì› * (1 + 0.05 )^2 = 1,102,500ì›\n",
      "\n",
      "ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 1,102,500ì› - 1,000,000ì› = 102,500ì› ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì˜µì…˜ Dì˜ ì£¼ì¥ì´ ì˜³ì§€ ì•ŠìŠµë‹ˆë‹¤.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ì¤‘ í™”íì˜ ì‹œê°„ê°€ì¹˜ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "A. ì›” ë³µë¦¬ì˜ ê²½ìš°, ë§¤ì›” ì ìš©ë˜ëŠ” ì´ììœ¨ì€ ì—°ê°„ ëª…ëª© ì´ììœ¨ì„ 1/12ë¡œ ë‚˜ëˆ„ì–´ ì‚°ì¶œí•œë‹¤.\n",
    "B. íˆ¬ì ì›ê¸ˆ ë° ê¸°íƒ€ ì¡°ê±´ì´ ë™ì¼í•  ê²½ìš°, ë‹¨ë¦¬ ë°©ì‹ë³´ë‹¤ ë³µë¦¬ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìê°€ ë” í¬ë‹¤.\n",
    "C. ì¼ì‹œë¶ˆë¡œ ì§€ê¸‰ë  ê¸ˆì•¡ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ ê°€ì¹˜ë¥¼ ì¼ì • ê¸°ê°„ ë™ì•ˆ í• ì¸ìœ¨ì„ ì ìš©í•´ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤.\n",
    "D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
    "    \"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
