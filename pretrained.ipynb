{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['term', 'context', 'questions'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('/root/KRX_LLM/data/finance_terms_context_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n",
      "['2ì°¨ ì‹œì¥(Secondary Market)', '5ì¼ì„ ', 'ABCP(Asset Backed Commercial Paper)', 'AMA(Auto Management Account)', 'At The Money(ATM, ì•³ ë” ë¨¸ë‹ˆ)', 'BIC', 'BIS(Bank for International Settlements, êµ­ì œê²°ì œì€í–‰)', 'Barclays Global Aggregate', 'Behavioral Finance', 'CAMELì§€ìˆ˜(ì¹´ë©œì§€ìˆ˜)']\n"
     ]
    }
   ],
   "source": [
    "terms = []\n",
    "\n",
    "for item in data:\n",
    "    terms.append(item['term'])\n",
    "\n",
    "print(len(terms))\n",
    "print(terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "env_path = os.path.join(os.getcwd(), '.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "hf_token = os.getenv(\"HF_TEAM_TOKEN\")\n",
    "print(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.47.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.691 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = 'KR-X-AI/krx-qwen2-7b-instruct-v4_m'\n",
    "max_seq_length = 2048\n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„ ì–¸\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ê³ ë¹ˆë„ê°€ê²©ê±°ë˜(HFT)' in terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    BlackRockì´ ë­ì•¼?\n",
      "     BlackRockì€ ì„¸ê³„ì—ì„œ ê°€ì¥ í° íˆ¬ìíšŒì‚¬ ì¤‘ í•˜ë‚˜ë¡œ, ì£¼ì‹, ì±„ê¶Œ, ë¶€ë™ì‚° ë“± ë‹¤ì–‘í•œ ìì‚°ì— ëŒ€í•œ íˆ¬ìë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤. 10ì¡° ë‹¬ëŸ¬ ì´ìƒì˜ ìì‚°ì„ ê´€ë¦¬í•˜ë©°, ê³ ê°ì˜ íˆ¬ì ëª©í‘œì™€ ìœ„í—˜ ìˆ˜ìš© ëŠ¥ë ¥ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ íˆ¬ì ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤. \n",
      "\n",
      "    BlackRockì€ ì—¬ëŸ¬ ë¶€ì„œë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆìœ¼ë©°, ê°ê°ì— íŠ¹í™”ëœ ì „ë¬¸ê°€ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶€ì„œì—ëŠ” ë¦¬ìŠ¤í¬ ê´€ë¦¬, í€ë“œ ê´€ë¦¬, íˆ¬ì ì—°êµ¬ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤.\n",
      "\n",
      "    BlackRockì˜ ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ëŠ” ì£¼ì‹ í€ë“œ, ì±„ê¶Œ í€ë“œ, ë¶€ë™ì‚° í€ë“œ, ì¸ë²„ìŠ¤ í€ë“œ ë“±ì„ ë¹„ë¡¯í•œ ë‹¤ì–‘í•œ íˆ¬ì ìƒí’ˆì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, ê³ ê°ë“¤ì´ ê°œì¸ì ìœ¼ë¡œ ì§ì ‘ íˆ¬ìí•  ë•Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ê³ ê°ë“¤ì´ ìì‹ ì˜ íˆ¬ì ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ë‹¤ì–‘í•œ íˆ¬ì ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "    BlackRockì˜ ì„±ê³µì€ ê·¸ë“¤ì˜ ì „ë¬¸ì ì¸ íŒ€ê³¼ ê³ ê° ì¤‘ì‹¬ì˜ ì ‘ê·¼ ë°©ì‹ì— ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ê³ ê°ì˜ íˆ¬ì ëª©í‘œë¥¼ ì´í•´í•˜ê³ , ê·¸ì— ë§ëŠ” ìµœì ì˜ íˆ¬ì ì „ëµì„ ì œê³µí•¨ìœ¼ë¡œì¨ ê³ ê°ë“¤ì—ê²Œ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"\n",
    "    BlackRockì´ ë­ì•¼?\n",
    "    \"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ë§¤ë„í—¤ì§€(Sell Hedge)ì˜ ì£¼ëœ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?\n",
      "\n",
      "A. ê°€ê²© ìƒìŠ¹ì„ í†µí•´ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´\n",
      "B. ë³´ìœ  ìì‚°ì˜ ê°€ê²© í•˜ë½ ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´\n",
      "C. ë‹¨ê¸° íˆ¬ì ìˆ˜ìµì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´\n",
      "D. ì‹œì¥ ì ìœ ìœ¨ì„ í™•ëŒ€í•˜ê¸° ìœ„í•´\n",
      "E. íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ë³€í™”ë¥¼ ìœ„í•´\n",
      "     \"\"\"\n",
      "    answer = \"B. ë³´ìœ  ìì‚°ì˜ ê°€ê²© í•˜ë½ ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´\"\n",
      "    return answer\n",
      "\n",
      "result = sell_hedge_objective()\n",
      "print(result)\n",
      "```\n",
      "Run complete, with result 'B. ë³´ìœ  ìì‚°ì˜ ê°€ê²© í•˜ë½ ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´'. The main purpose of a sell hedge is to reduce the risk of depreciation in the value of held assets.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"\n",
    "    ë§¤ë„í—¤ì§€(Sell Hedge)ì˜ ì£¼ëœ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "A. ê°€ê²© ìƒìŠ¹ì„ í†µí•´ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´\n",
    "B. ë³´ìœ  ìì‚°ì˜ ê°€ê²© í•˜ë½ ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´\n",
    "C. ë‹¨ê¸° íˆ¬ì ìˆ˜ìµì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´\n",
    "D. ì‹œì¥ ì ìœ ìœ¨ì„ í™•ëŒ€í•˜ê¸° ìœ„í•´\n",
    "E. íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ë³€í™”ë¥¼ ìœ„í•´\n",
    "    \"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ë§¤ë„í—¤ì§€\n",
      "í† í°: ['Ã«Â§Â¤', 'Ã«Ä±Ä¦', 'Ã­Ä¹Â¤', 'Ã¬Â§Ä¢']\n",
      "í† í° ìˆ˜: 4\n"
     ]
    }
   ],
   "source": [
    "def show_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"ì›ë¬¸: {text}\")\n",
    "    print(f\"í† í°: {tokens}\")\n",
    "    print(f\"í† í° ìˆ˜: {len(tokens)}\")\n",
    "\n",
    "test_text = terms[250]\n",
    "show_tokens(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë§¤ë„í—¤ì§€\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text):\n",
    "    return tokenizer.encode(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "test_text = terms[250]\n",
    "encoded = encode_text(test_text)\n",
    "decoded = tokenizer.decode(encoded[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(terms)\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì›ë¬¸: ë§¤ë„í—¤ì§€\n",
      "í† í°: ['ë§¤ë„í—¤ì§€']\n",
      "í† í° ìˆ˜: 1\n"
     ]
    }
   ],
   "source": [
    "def show_tokens(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"ì›ë¬¸: {text}\")\n",
    "    print(f\"í† í°: {tokens}\")\n",
    "    print(f\"í† í° ìˆ˜: {len(tokens)}\")\n",
    "\n",
    "test_text = terms[250]\n",
    "show_tokens(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ë‹¤ìŒ ì¤‘ í™”íì˜ ì‹œê°„ê°€ì¹˜ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€?\n",
      "\n",
      "A. ì›” ë³µë¦¬ì˜ ê²½ìš°, ë§¤ì›” ì ìš©ë˜ëŠ” ì´ììœ¨ì€ ì—°ê°„ ëª…ëª© ì´ììœ¨ì„ 1/12ë¡œ ë‚˜ëˆ„ì–´ ì‚°ì¶œí•œë‹¤.\n",
      "B. íˆ¬ì ì›ê¸ˆ ë° ê¸°íƒ€ ì¡°ê±´ì´ ë™ì¼í•  ê²½ìš°, ë‹¨ë¦¬ ë°©ì‹ë³´ë‹¤ ë³µë¦¬ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìê°€ ë” í¬ë‹¤.\n",
      "C. ì¼ì‹œë¶ˆë¡œ ì§€ê¸‰ë  ê¸ˆì•¡ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ ê°€ì¹˜ë¥¼ ì¼ì • ê¸°ê°„ ë™ì•ˆ í• ì¸ìœ¨ì„ ì ìš©í•´ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤.\n",
      "D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
      "     \"\"\"\n",
      "<|im_start|>assistant: D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
      "\n",
      "ì´ ì„ íƒì§€ê°€ ì˜³ì§€ ì•Šì€ ì´ìœ ëŠ” 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í•˜ë©´ ë§Œê¸° ì‹œì—ëŠ” ì›ê¸ˆê³¼ ì´ìë¥¼ í•©ì¹œ ì´ê¸ˆì•¡ì´ 1,000,000ì› * (1 + 5%)^2 = 1,102,500ì›ì´ë¯€ë¡œ ì´ìì˜ ê¸ˆì•¡ì€ 1,102,500ì› - 1,000,000ì› = 102,500ì›ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ìê°€ 100,000ì›ì´ ì•„ë‹ˆë¼ 102,500ì›ì…ë‹ˆë‹¤.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"\n",
    "    ë‹¤ìŒ ì¤‘ í™”íì˜ ì‹œê°„ê°€ì¹˜ì— ê´€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì€ ë¬´ì—‡ì¸ê°€?\n",
    "\n",
    "A. ì›” ë³µë¦¬ì˜ ê²½ìš°, ë§¤ì›” ì ìš©ë˜ëŠ” ì´ììœ¨ì€ ì—°ê°„ ëª…ëª© ì´ììœ¨ì„ 1/12ë¡œ ë‚˜ëˆ„ì–´ ì‚°ì¶œí•œë‹¤.\n",
    "B. íˆ¬ì ì›ê¸ˆ ë° ê¸°íƒ€ ì¡°ê±´ì´ ë™ì¼í•  ê²½ìš°, ë‹¨ë¦¬ ë°©ì‹ë³´ë‹¤ ë³µë¦¬ ë°©ì‹ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìê°€ ë” í¬ë‹¤.\n",
    "C. ì¼ì‹œë¶ˆë¡œ ì§€ê¸‰ë  ê¸ˆì•¡ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ ê°€ì¹˜ë¥¼ ì¼ì • ê¸°ê°„ ë™ì•ˆ í• ì¸ìœ¨ì„ ì ìš©í•´ ì‚°ì¶œí•  ìˆ˜ ìˆë‹¤.\n",
    "D. 1,000,000ì›ì„ ì—° 5% ë³µë¦¬ë¡œ 2ë…„ ë™ì•ˆ ì˜ˆì¹˜í–ˆì„ ê²½ìš°, ë§Œê¸°ì— ë°›ì„ ì„¸ì „ ì´ìëŠ” 100,000ì›ì´ë‹¤.\n",
    "    \"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 400, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ë§¤ë„ í—¤ì§€ê°€ ë­ì•¼?\n",
      "     : ë§¤ë„ í—¤ì§€ëŠ” ì£¼ì‹ì„ ë§¤ë„í•˜ì—¬ ê°€ê²©ì´ í•˜ë½í•˜ëŠ” ê²ƒì„ ì´ìš©í•´ ìˆ˜ìµì„ ì–»ëŠ” íˆ¬ì ì „ëµì…ë‹ˆë‹¤.\n",
      "     ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì‹ ê°€ê²©ì´ ìƒìŠ¹í•˜ê³  ìˆëŠ” ìƒí™©ì—ì„œ ì£¼ì‹ì„ ë§¤ìˆ˜í•œ ê²½ìš°, ê°€ê²©ì´ ê³„ì† ìƒìŠ¹í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì£¼ì‹ ê°€ê²©ì´ í•˜ë½í•˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë ¤ ì£¼ì‹ì„ ë§¤ë„í•˜ì—¬ ìˆ˜ìµì„ ì–»ëŠ” ê²ƒì´ ë§¤ë„ í—¤ì§€ ì „ëµì…ë‹ˆë‹¤.\n",
      "     ì´ì™€ ë°˜ëŒ€ë¡œ ë§¤ìˆ˜ í—¤ì§€ëŠ” ì£¼ì‹ ê°€ê²©ì´ ìƒìŠ¹í•  ê°€ëŠ¥ì„±ì„ ê¸°ëŒ€í•˜ê³  ì£¼ì‹ì„ ë§¤ìˆ˜í•œ í›„ ê°€ê²©ì´ ìƒìŠ¹í•˜ë©´ ì´ìµì„ ì–»ëŠ” ì „ëµì…ë‹ˆë‹¤.\n",
      "\n",
      "    ë§¤ë„ í—¤ì§€ ì „ëµì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¥¼ ë”°ë¦…ë‹ˆë‹¤:\n",
      "    1. ì£¼ì‹ì„ ë§¤ìˆ˜í•œ í›„ ì£¼ì‹ ê°€ê²©ì´ ìƒìŠ¹í•  ê°€ëŠ¥ì„±ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.\n",
      "    2. ì£¼ì‹ ê°€ê²©ì´ ìƒìŠ¹í•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.\n",
      "    3. ì£¼ì‹ ê°€ê²©ì´ ìƒìŠ¹í•˜ë©´ ì£¼ì‹ì„ ë§¤ë„í•˜ì—¬ ì´ìµì„ ì–»ìŠµë‹ˆë‹¤.\n",
      "    4. ì£¼ì‹ ê°€ê²©ì´ ìƒìŠ¹í•˜ì§€ ì•Šì„ ê²½ìš°, ì£¼ì‹ì„ ê³„ì† ë³´ìœ í•˜ëŠ” ê²ƒìœ¼ë¡œ ì´ìµì„ ì–»ìŠµë‹ˆë‹¤.\n",
      "    5. ì£¼ì‹ ê°€ê²©ì´ í•˜ë½í•˜ë©´ ì£¼ì‹ì„ ë§¤ë„í•˜ì—¬ ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
      "    ë§¤ë„ í—¤ì§€ ì „ëµì€ ì£¼ì‹ ê°€ê²©ì˜ ìƒìŠ¹ ê°€ëŠ¥ì„±ì„ ê¸°ëŒ€í•˜ë©´ì„œ ì£¼ì‹ ê°€ê²©ì´ í•˜ë½í•  ê²½ìš° ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ì–´ì ì¸ íˆ¬ì ì „ëµì…ë‹ˆë‹¤. \n",
      "\n",
      "    ë§¤ë„ í—¤ì§€ ì „ëµì„ ì‚¬ìš©í•˜ë ¤ë©´ ì£¼ì‹ ì‹œì¥ì˜ ë™í–¥ì„ ì£¼ì‹œí•˜ê³ , ì£¼ì‹ ê°€ê²©ì˜ ë³€ë™ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì˜ í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ì£¼ì‹ì„ ë§¤ë„í•˜ëŠ” ì‹œì ê³¼ ë§¤ìˆ˜í•œ ì£¼ì‹ì„ ë§¤ë„í•˜ëŠ” ì‹œì ì— ëŒ€í•œ ê³„íšì„ ì„¸ì›Œì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì£¼ì‹ íˆ¬ìì—ì„œ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ì‘ìš©í•˜ë©°, ì£¼ì‹ íˆ¬ìì˜ ì„±ê³µì„ ìœ„í•œ í•„ìˆ˜ì ì¸ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
      "    \n",
      "    ê·¸ëŸ¼ ì£¼ì‹ í—¤ì§€ì™€ ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ì–´ë–»ê²Œ êµ¬í˜„í• ê¹Œìš”?\n",
      "    ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ëŠ” ì£¼ì‹ í—¤ì§€ ì „ëµì„ ì ìš©í•˜ì—¬ íˆ¬ì ìœ„í—˜ì„ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬í˜„í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¥¼ ë”°ë¥´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤:\n",
      "\n",
      "    1. íˆ¬ì ëª©í‘œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ì£¼ìš” ëª©í‘œëŠ” íˆ¬ì ìœ„í—˜ì„ ê´€ë¦¬í•˜ê³  íˆ¬ì ìˆ˜ìµë¥ ì„ ë†’ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
      "    2. íˆ¬ì ìì‚°ì„ ë¶„ì‚°í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì£¼ì‹ì´ë‚˜ ê¸°íƒ€ íˆ¬ì ìì‚°ì„ í¬í•¨í•˜ì—¬ íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ í•œ ì¢…ëª©ì˜ ê°€ì¹˜ í•˜ë½ìœ¼ë¡œ ì¸í•œ ì†ì‹¤ì„ ë‹¤ë¥¸ ì¢…ëª©ì˜ ê°€ì¹˜ ì¦ê°€ë¡œ ë³´ì™„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "    3. ì£¼ì‹ í—¤ì§€ ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤. ì£¼ì‹ í—¤ì§€ ì „ëµì„ ì ìš©í•˜ì—¬ íŠ¹ì • ì£¼ì‹ì˜ ê°€ì¹˜ í•˜ë½ ìœ„í—˜ì„ ì¤„ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì£¼ì‹ì„ ë§¤ìˆ˜í•œ í›„ í•´ë‹¹ ì£¼ì‹ì˜ ê°€ì¹˜ê°€ í•˜ë½í•  ê²½ìš° ë§¤ë„ í—¤ì§€ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.\n",
      "    4. ì£¼ê¸°ì ìœ¼ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í‰ê°€í•˜ê³  ì¡°ì •í•©ë‹ˆë‹¤. ì£¼ì‹ ì‹œì¥ì˜ ë™í–¥ì´ë‚˜ ê°œì¸ì˜ íˆ¬ì ëª©í‘œê°€ ë³€ê²½ë˜ë©´ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
      "    5. ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ì§€ì†ì ì¸ íˆ¬ì êµìœ¡ì„ ë°›ê³ , ì£¼ì‹ ì‹œì¥ì˜ ë™í–¥ì„ ì£¼ì‹œí•˜ë©°, íˆ¬ì ì „ëµì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
      "    \n",
      "    ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì£¼ì‹ í—¤ì§€ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬í˜„í•˜ë©´ íˆ¬ì ìœ„í—˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  íˆ¬ì ìˆ˜ìµë¥ ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"\n",
    "    ë§¤ë„ í—¤ì§€ê°€ ë­ì•¼?\n",
    "    \"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
