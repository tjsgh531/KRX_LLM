{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnZuU7PVtZF"
      },
      "source": [
        "# KRX-Bench 언어 모델 Instruction Tuning 튜토리얼\n",
        "- Unsloth와 금융 도메인 합성 데이터셋을 활용하여 효율적으로 모델을 학습시키는 튜토리얼입니다.\n",
        "- Unsloth는 적은 컴퓨팅 자원으로도 모델을 효율적으로 빠르게 학습시킬 수 있는 방법을 제공하는 라이브러리입니다.\n",
        "\n",
        "## 튜토리얼 환경 및 세부사항\n",
        "- 모델: [Meta-Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)\n",
        "- 데이터셋: [amphora/krx-sample-instructions](https://huggingface.co/datasets/amphora/krx-sample-instructions)\n",
        "- 학습 툴: [Unsloth](https://github.com/unslothai/unsloth)\n",
        "- 학습 방법 : Supervised Fine-tuning with QLoRA(4bit)\n",
        "- 학습 컴퓨팅 환경: Google Colab T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjizHH4vVtZG"
      },
      "source": [
        "## 1. Unsloth 설치 및 학습 환경 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S4DqZvT6VtZG"
      },
      "outputs": [],
      "source": [
        "# # Unsloth 설치 및 최신 버전 업그레이드\n",
        "# !pip install unsloth\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install trl bitsandbytes triton xformers peft\n",
        "\n",
        "# # GPU가 softcapping을 지원하는 경우, Flash Attention 2 설치\n",
        "# import torch\n",
        "# if torch.cuda.get_device_capability()[0] >= 8:\n",
        "#     !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNfOabrCVtZH"
      },
      "source": [
        "## 2. 모델 및 데이터셋 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sK96swGiVtZH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.10.7: Fast Qwen2 patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.69 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.6. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
            "Please update transformers, TRL and unsloth via:\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None # None으로 지정할 경우 해당 컴퓨팅 유닛에 알맞은 dtype으로 저장됩니다. Tesla T4와 V100의 경우에는 Float16, Ampere+ 이상의 경우에는 Bfloat16으로 설정됩니다.\n",
        "load_in_4bit = True # 메모리 사용량을 줄이기 위해서는 4bit 양자화를 사용하실 것을 권장합니다.\n",
        "\n",
        "# 모델 및 토크나이저 선언\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2-7B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    token = \"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\", # gated model을 사용할 경우 허깅페이스 토큰을 입력해주시길 바라겠습니다.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model\n",
            "model.embed_tokens\n",
            "model.layers\n",
            "model.layers.0\n",
            "model.layers.0.self_attn\n",
            "model.layers.0.self_attn.q_proj\n",
            "model.layers.0.self_attn.k_proj\n",
            "model.layers.0.self_attn.v_proj\n",
            "model.layers.0.self_attn.o_proj\n",
            "model.layers.0.self_attn.rotary_emb\n",
            "model.layers.0.mlp\n",
            "model.layers.0.mlp.gate_proj\n",
            "model.layers.0.mlp.up_proj\n",
            "model.layers.0.mlp.down_proj\n",
            "model.layers.0.mlp.act_fn\n",
            "model.layers.0.input_layernorm\n",
            "model.layers.0.post_attention_layernorm\n",
            "model.layers.1\n",
            "model.layers.1.self_attn\n",
            "model.layers.1.self_attn.q_proj\n",
            "model.layers.1.self_attn.k_proj\n",
            "model.layers.1.self_attn.v_proj\n",
            "model.layers.1.self_attn.o_proj\n",
            "model.layers.1.self_attn.rotary_emb\n",
            "model.layers.1.mlp\n",
            "model.layers.1.mlp.gate_proj\n",
            "model.layers.1.mlp.up_proj\n",
            "model.layers.1.mlp.down_proj\n",
            "model.layers.1.mlp.act_fn\n",
            "model.layers.1.input_layernorm\n",
            "model.layers.1.post_attention_layernorm\n",
            "model.layers.2\n",
            "model.layers.2.self_attn\n",
            "model.layers.2.self_attn.q_proj\n",
            "model.layers.2.self_attn.k_proj\n",
            "model.layers.2.self_attn.v_proj\n",
            "model.layers.2.self_attn.o_proj\n",
            "model.layers.2.self_attn.rotary_emb\n",
            "model.layers.2.mlp\n",
            "model.layers.2.mlp.gate_proj\n",
            "model.layers.2.mlp.up_proj\n",
            "model.layers.2.mlp.down_proj\n",
            "model.layers.2.mlp.act_fn\n",
            "model.layers.2.input_layernorm\n",
            "model.layers.2.post_attention_layernorm\n",
            "model.layers.3\n",
            "model.layers.3.self_attn\n",
            "model.layers.3.self_attn.q_proj\n",
            "model.layers.3.self_attn.k_proj\n",
            "model.layers.3.self_attn.v_proj\n",
            "model.layers.3.self_attn.o_proj\n",
            "model.layers.3.self_attn.rotary_emb\n",
            "model.layers.3.mlp\n",
            "model.layers.3.mlp.gate_proj\n",
            "model.layers.3.mlp.up_proj\n",
            "model.layers.3.mlp.down_proj\n",
            "model.layers.3.mlp.act_fn\n",
            "model.layers.3.input_layernorm\n",
            "model.layers.3.post_attention_layernorm\n",
            "model.layers.4\n",
            "model.layers.4.self_attn\n",
            "model.layers.4.self_attn.q_proj\n",
            "model.layers.4.self_attn.k_proj\n",
            "model.layers.4.self_attn.v_proj\n",
            "model.layers.4.self_attn.o_proj\n",
            "model.layers.4.self_attn.rotary_emb\n",
            "model.layers.4.mlp\n",
            "model.layers.4.mlp.gate_proj\n",
            "model.layers.4.mlp.up_proj\n",
            "model.layers.4.mlp.down_proj\n",
            "model.layers.4.mlp.act_fn\n",
            "model.layers.4.input_layernorm\n",
            "model.layers.4.post_attention_layernorm\n",
            "model.layers.5\n",
            "model.layers.5.self_attn\n",
            "model.layers.5.self_attn.q_proj\n",
            "model.layers.5.self_attn.k_proj\n",
            "model.layers.5.self_attn.v_proj\n",
            "model.layers.5.self_attn.o_proj\n",
            "model.layers.5.self_attn.rotary_emb\n",
            "model.layers.5.mlp\n",
            "model.layers.5.mlp.gate_proj\n",
            "model.layers.5.mlp.up_proj\n",
            "model.layers.5.mlp.down_proj\n",
            "model.layers.5.mlp.act_fn\n",
            "model.layers.5.input_layernorm\n",
            "model.layers.5.post_attention_layernorm\n",
            "model.layers.6\n",
            "model.layers.6.self_attn\n",
            "model.layers.6.self_attn.q_proj\n",
            "model.layers.6.self_attn.k_proj\n",
            "model.layers.6.self_attn.v_proj\n",
            "model.layers.6.self_attn.o_proj\n",
            "model.layers.6.self_attn.rotary_emb\n",
            "model.layers.6.mlp\n",
            "model.layers.6.mlp.gate_proj\n",
            "model.layers.6.mlp.up_proj\n",
            "model.layers.6.mlp.down_proj\n",
            "model.layers.6.mlp.act_fn\n",
            "model.layers.6.input_layernorm\n",
            "model.layers.6.post_attention_layernorm\n",
            "model.layers.7\n",
            "model.layers.7.self_attn\n",
            "model.layers.7.self_attn.q_proj\n",
            "model.layers.7.self_attn.k_proj\n",
            "model.layers.7.self_attn.v_proj\n",
            "model.layers.7.self_attn.o_proj\n",
            "model.layers.7.self_attn.rotary_emb\n",
            "model.layers.7.mlp\n",
            "model.layers.7.mlp.gate_proj\n",
            "model.layers.7.mlp.up_proj\n",
            "model.layers.7.mlp.down_proj\n",
            "model.layers.7.mlp.act_fn\n",
            "model.layers.7.input_layernorm\n",
            "model.layers.7.post_attention_layernorm\n",
            "model.layers.8\n",
            "model.layers.8.self_attn\n",
            "model.layers.8.self_attn.q_proj\n",
            "model.layers.8.self_attn.k_proj\n",
            "model.layers.8.self_attn.v_proj\n",
            "model.layers.8.self_attn.o_proj\n",
            "model.layers.8.self_attn.rotary_emb\n",
            "model.layers.8.mlp\n",
            "model.layers.8.mlp.gate_proj\n",
            "model.layers.8.mlp.up_proj\n",
            "model.layers.8.mlp.down_proj\n",
            "model.layers.8.mlp.act_fn\n",
            "model.layers.8.input_layernorm\n",
            "model.layers.8.post_attention_layernorm\n",
            "model.layers.9\n",
            "model.layers.9.self_attn\n",
            "model.layers.9.self_attn.q_proj\n",
            "model.layers.9.self_attn.k_proj\n",
            "model.layers.9.self_attn.v_proj\n",
            "model.layers.9.self_attn.o_proj\n",
            "model.layers.9.self_attn.rotary_emb\n",
            "model.layers.9.mlp\n",
            "model.layers.9.mlp.gate_proj\n",
            "model.layers.9.mlp.up_proj\n",
            "model.layers.9.mlp.down_proj\n",
            "model.layers.9.mlp.act_fn\n",
            "model.layers.9.input_layernorm\n",
            "model.layers.9.post_attention_layernorm\n",
            "model.layers.10\n",
            "model.layers.10.self_attn\n",
            "model.layers.10.self_attn.q_proj\n",
            "model.layers.10.self_attn.k_proj\n",
            "model.layers.10.self_attn.v_proj\n",
            "model.layers.10.self_attn.o_proj\n",
            "model.layers.10.self_attn.rotary_emb\n",
            "model.layers.10.mlp\n",
            "model.layers.10.mlp.gate_proj\n",
            "model.layers.10.mlp.up_proj\n",
            "model.layers.10.mlp.down_proj\n",
            "model.layers.10.mlp.act_fn\n",
            "model.layers.10.input_layernorm\n",
            "model.layers.10.post_attention_layernorm\n",
            "model.layers.11\n",
            "model.layers.11.self_attn\n",
            "model.layers.11.self_attn.q_proj\n",
            "model.layers.11.self_attn.k_proj\n",
            "model.layers.11.self_attn.v_proj\n",
            "model.layers.11.self_attn.o_proj\n",
            "model.layers.11.self_attn.rotary_emb\n",
            "model.layers.11.mlp\n",
            "model.layers.11.mlp.gate_proj\n",
            "model.layers.11.mlp.up_proj\n",
            "model.layers.11.mlp.down_proj\n",
            "model.layers.11.mlp.act_fn\n",
            "model.layers.11.input_layernorm\n",
            "model.layers.11.post_attention_layernorm\n",
            "model.layers.12\n",
            "model.layers.12.self_attn\n",
            "model.layers.12.self_attn.q_proj\n",
            "model.layers.12.self_attn.k_proj\n",
            "model.layers.12.self_attn.v_proj\n",
            "model.layers.12.self_attn.o_proj\n",
            "model.layers.12.self_attn.rotary_emb\n",
            "model.layers.12.mlp\n",
            "model.layers.12.mlp.gate_proj\n",
            "model.layers.12.mlp.up_proj\n",
            "model.layers.12.mlp.down_proj\n",
            "model.layers.12.mlp.act_fn\n",
            "model.layers.12.input_layernorm\n",
            "model.layers.12.post_attention_layernorm\n",
            "model.layers.13\n",
            "model.layers.13.self_attn\n",
            "model.layers.13.self_attn.q_proj\n",
            "model.layers.13.self_attn.k_proj\n",
            "model.layers.13.self_attn.v_proj\n",
            "model.layers.13.self_attn.o_proj\n",
            "model.layers.13.self_attn.rotary_emb\n",
            "model.layers.13.mlp\n",
            "model.layers.13.mlp.gate_proj\n",
            "model.layers.13.mlp.up_proj\n",
            "model.layers.13.mlp.down_proj\n",
            "model.layers.13.mlp.act_fn\n",
            "model.layers.13.input_layernorm\n",
            "model.layers.13.post_attention_layernorm\n",
            "model.layers.14\n",
            "model.layers.14.self_attn\n",
            "model.layers.14.self_attn.q_proj\n",
            "model.layers.14.self_attn.k_proj\n",
            "model.layers.14.self_attn.v_proj\n",
            "model.layers.14.self_attn.o_proj\n",
            "model.layers.14.self_attn.rotary_emb\n",
            "model.layers.14.mlp\n",
            "model.layers.14.mlp.gate_proj\n",
            "model.layers.14.mlp.up_proj\n",
            "model.layers.14.mlp.down_proj\n",
            "model.layers.14.mlp.act_fn\n",
            "model.layers.14.input_layernorm\n",
            "model.layers.14.post_attention_layernorm\n",
            "model.layers.15\n",
            "model.layers.15.self_attn\n",
            "model.layers.15.self_attn.q_proj\n",
            "model.layers.15.self_attn.k_proj\n",
            "model.layers.15.self_attn.v_proj\n",
            "model.layers.15.self_attn.o_proj\n",
            "model.layers.15.self_attn.rotary_emb\n",
            "model.layers.15.mlp\n",
            "model.layers.15.mlp.gate_proj\n",
            "model.layers.15.mlp.up_proj\n",
            "model.layers.15.mlp.down_proj\n",
            "model.layers.15.mlp.act_fn\n",
            "model.layers.15.input_layernorm\n",
            "model.layers.15.post_attention_layernorm\n",
            "model.layers.16\n",
            "model.layers.16.self_attn\n",
            "model.layers.16.self_attn.q_proj\n",
            "model.layers.16.self_attn.k_proj\n",
            "model.layers.16.self_attn.v_proj\n",
            "model.layers.16.self_attn.o_proj\n",
            "model.layers.16.self_attn.rotary_emb\n",
            "model.layers.16.mlp\n",
            "model.layers.16.mlp.gate_proj\n",
            "model.layers.16.mlp.up_proj\n",
            "model.layers.16.mlp.down_proj\n",
            "model.layers.16.mlp.act_fn\n",
            "model.layers.16.input_layernorm\n",
            "model.layers.16.post_attention_layernorm\n",
            "model.layers.17\n",
            "model.layers.17.self_attn\n",
            "model.layers.17.self_attn.q_proj\n",
            "model.layers.17.self_attn.k_proj\n",
            "model.layers.17.self_attn.v_proj\n",
            "model.layers.17.self_attn.o_proj\n",
            "model.layers.17.self_attn.rotary_emb\n",
            "model.layers.17.mlp\n",
            "model.layers.17.mlp.gate_proj\n",
            "model.layers.17.mlp.up_proj\n",
            "model.layers.17.mlp.down_proj\n",
            "model.layers.17.mlp.act_fn\n",
            "model.layers.17.input_layernorm\n",
            "model.layers.17.post_attention_layernorm\n",
            "model.layers.18\n",
            "model.layers.18.self_attn\n",
            "model.layers.18.self_attn.q_proj\n",
            "model.layers.18.self_attn.k_proj\n",
            "model.layers.18.self_attn.v_proj\n",
            "model.layers.18.self_attn.o_proj\n",
            "model.layers.18.self_attn.rotary_emb\n",
            "model.layers.18.mlp\n",
            "model.layers.18.mlp.gate_proj\n",
            "model.layers.18.mlp.up_proj\n",
            "model.layers.18.mlp.down_proj\n",
            "model.layers.18.mlp.act_fn\n",
            "model.layers.18.input_layernorm\n",
            "model.layers.18.post_attention_layernorm\n",
            "model.layers.19\n",
            "model.layers.19.self_attn\n",
            "model.layers.19.self_attn.q_proj\n",
            "model.layers.19.self_attn.k_proj\n",
            "model.layers.19.self_attn.v_proj\n",
            "model.layers.19.self_attn.o_proj\n",
            "model.layers.19.self_attn.rotary_emb\n",
            "model.layers.19.mlp\n",
            "model.layers.19.mlp.gate_proj\n",
            "model.layers.19.mlp.up_proj\n",
            "model.layers.19.mlp.down_proj\n",
            "model.layers.19.mlp.act_fn\n",
            "model.layers.19.input_layernorm\n",
            "model.layers.19.post_attention_layernorm\n",
            "model.layers.20\n",
            "model.layers.20.self_attn\n",
            "model.layers.20.self_attn.q_proj\n",
            "model.layers.20.self_attn.k_proj\n",
            "model.layers.20.self_attn.v_proj\n",
            "model.layers.20.self_attn.o_proj\n",
            "model.layers.20.self_attn.rotary_emb\n",
            "model.layers.20.mlp\n",
            "model.layers.20.mlp.gate_proj\n",
            "model.layers.20.mlp.up_proj\n",
            "model.layers.20.mlp.down_proj\n",
            "model.layers.20.mlp.act_fn\n",
            "model.layers.20.input_layernorm\n",
            "model.layers.20.post_attention_layernorm\n",
            "model.layers.21\n",
            "model.layers.21.self_attn\n",
            "model.layers.21.self_attn.q_proj\n",
            "model.layers.21.self_attn.k_proj\n",
            "model.layers.21.self_attn.v_proj\n",
            "model.layers.21.self_attn.o_proj\n",
            "model.layers.21.self_attn.rotary_emb\n",
            "model.layers.21.mlp\n",
            "model.layers.21.mlp.gate_proj\n",
            "model.layers.21.mlp.up_proj\n",
            "model.layers.21.mlp.down_proj\n",
            "model.layers.21.mlp.act_fn\n",
            "model.layers.21.input_layernorm\n",
            "model.layers.21.post_attention_layernorm\n",
            "model.layers.22\n",
            "model.layers.22.self_attn\n",
            "model.layers.22.self_attn.q_proj\n",
            "model.layers.22.self_attn.k_proj\n",
            "model.layers.22.self_attn.v_proj\n",
            "model.layers.22.self_attn.o_proj\n",
            "model.layers.22.self_attn.rotary_emb\n",
            "model.layers.22.mlp\n",
            "model.layers.22.mlp.gate_proj\n",
            "model.layers.22.mlp.up_proj\n",
            "model.layers.22.mlp.down_proj\n",
            "model.layers.22.mlp.act_fn\n",
            "model.layers.22.input_layernorm\n",
            "model.layers.22.post_attention_layernorm\n",
            "model.layers.23\n",
            "model.layers.23.self_attn\n",
            "model.layers.23.self_attn.q_proj\n",
            "model.layers.23.self_attn.k_proj\n",
            "model.layers.23.self_attn.v_proj\n",
            "model.layers.23.self_attn.o_proj\n",
            "model.layers.23.self_attn.rotary_emb\n",
            "model.layers.23.mlp\n",
            "model.layers.23.mlp.gate_proj\n",
            "model.layers.23.mlp.up_proj\n",
            "model.layers.23.mlp.down_proj\n",
            "model.layers.23.mlp.act_fn\n",
            "model.layers.23.input_layernorm\n",
            "model.layers.23.post_attention_layernorm\n",
            "model.layers.24\n",
            "model.layers.24.self_attn\n",
            "model.layers.24.self_attn.q_proj\n",
            "model.layers.24.self_attn.k_proj\n",
            "model.layers.24.self_attn.v_proj\n",
            "model.layers.24.self_attn.o_proj\n",
            "model.layers.24.self_attn.rotary_emb\n",
            "model.layers.24.mlp\n",
            "model.layers.24.mlp.gate_proj\n",
            "model.layers.24.mlp.up_proj\n",
            "model.layers.24.mlp.down_proj\n",
            "model.layers.24.mlp.act_fn\n",
            "model.layers.24.input_layernorm\n",
            "model.layers.24.post_attention_layernorm\n",
            "model.layers.25\n",
            "model.layers.25.self_attn\n",
            "model.layers.25.self_attn.q_proj\n",
            "model.layers.25.self_attn.k_proj\n",
            "model.layers.25.self_attn.v_proj\n",
            "model.layers.25.self_attn.o_proj\n",
            "model.layers.25.self_attn.rotary_emb\n",
            "model.layers.25.mlp\n",
            "model.layers.25.mlp.gate_proj\n",
            "model.layers.25.mlp.up_proj\n",
            "model.layers.25.mlp.down_proj\n",
            "model.layers.25.mlp.act_fn\n",
            "model.layers.25.input_layernorm\n",
            "model.layers.25.post_attention_layernorm\n",
            "model.layers.26\n",
            "model.layers.26.self_attn\n",
            "model.layers.26.self_attn.q_proj\n",
            "model.layers.26.self_attn.k_proj\n",
            "model.layers.26.self_attn.v_proj\n",
            "model.layers.26.self_attn.o_proj\n",
            "model.layers.26.self_attn.rotary_emb\n",
            "model.layers.26.mlp\n",
            "model.layers.26.mlp.gate_proj\n",
            "model.layers.26.mlp.up_proj\n",
            "model.layers.26.mlp.down_proj\n",
            "model.layers.26.mlp.act_fn\n",
            "model.layers.26.input_layernorm\n",
            "model.layers.26.post_attention_layernorm\n",
            "model.layers.27\n",
            "model.layers.27.self_attn\n",
            "model.layers.27.self_attn.q_proj\n",
            "model.layers.27.self_attn.k_proj\n",
            "model.layers.27.self_attn.v_proj\n",
            "model.layers.27.self_attn.o_proj\n",
            "model.layers.27.self_attn.rotary_emb\n",
            "model.layers.27.mlp\n",
            "model.layers.27.mlp.gate_proj\n",
            "model.layers.27.mlp.up_proj\n",
            "model.layers.27.mlp.down_proj\n",
            "model.layers.27.mlp.act_fn\n",
            "model.layers.27.input_layernorm\n",
            "model.layers.27.post_attention_layernorm\n",
            "model.norm\n",
            "lm_head\n"
          ]
        }
      ],
      "source": [
        "def print_submodules(model, prefix=''):\n",
        "    for name, submodule in model.named_children():\n",
        "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
        "        print(full_name)\n",
        "        print_submodules(submodule, full_name)\n",
        "\n",
        "# Run the function\n",
        "print_submodules(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u2mlY9pMVtZH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Not an error, but Unsloth cannot patch MLP layers with our manual autograd engine since either LoRA adapters\n",
            "are not enabled or a bias term (like in Qwen) is used.\n",
            "Unsloth 2024.10.7 patched 28 layers with 0 QKV layers, 28 O layers and 0 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "# LoRA Adapter 선언\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256, # 0을 넘는 숫자를 선택하세요. 8, 16, 32, 64, 128이 추천됩니다.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # target module도 적절하게 조정할 수 있습니다.\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # 어떤 값이든 사용될 수 있지만, 0으로 최적화되어 있습니다.\n",
        "    bias = \"none\", # 어떤 값이든 사용될 수 있지만, \"none\"으로 최적화되어 있습니다.\n",
        "    use_gradient_checkpointing = \"unsloth\", # 매우 긴 context에 대해 True 또는 \"unsloth\"를 사용하십시오.\n",
        "    random_state = 42,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loDn97x9VtZH"
      },
      "source": [
        "## 3. 데이터셋 전처리\n",
        "데이터셋은 한국어 금융 합성 데이터셋인 `amphora/krx-sample-instruction`를 사용하였고, 데이터셋 템플릿으로는 Alpaca prompt에서 input을 제거한 형태의 프롬프트 템플릿을 사용하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jEDXXvLTVtZH"
      },
      "outputs": [],
      "source": [
        "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"prompt\"]\n",
        "    outputs = examples[\"response\"]\n",
        "    texts = []\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        text = prompt_format.format(instruction, output) + EOS_TOKEN # 마지막에 eos token을 추가해줌으로써 모델이 출력을 끝마칠 수 있게 만들어 줍니다.\n",
        "        texts.append(text)\n",
        "    return { \"formatted_text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"amphora/krx-sample-instructions\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': '음악산업의 디지털화가 진행됨에 따라, 전통적인 음반 중심의 수익 모델에서 디지털 음악 서비스 중심으로 변화했음을 설명하는데, 이러한 변화가 이루어진 주된 원인은 무엇이며, 이에 따라 업계가 어떻게 대응하고 있는지 구체적인 예를 들어 설명하시오.',\n",
              " 'response': '음악산업의 디지털화 주된 원인은 인터넷과 мобиль 서비스의 발달로 인해 소비자들이 음악을 접근하고 소비하는 방식이 변화했기 때문입니다. 디지털 플랫폼의 등장으로 음원 스트리밍 서비스(예: 멜론, 스포티파이 등)가 보편화되면서, 소비자들은 과거의 음반 구매 대신 구독 기반의 서비스를 통해 음악에 쉽게 접근하게 되었습니다.\\n\\n이에 따라 음악업계는 다양한 대응 전략을 마련하고 있습니다. 예를 들어, 아티스트들은 음원 스트리밍 수익을 최적화하기 위해 SNS와 유튜브를 활용한 마케팅을 강화하고 있으며, 라이브 공연 및 팬미팅과 같은 오프라인 이벤트를 확대하여 추가 수익을 창출하고 있습니다. 또한, 일부 음악 레이블은 아티스트와의 협업을 통해 독점 콘텐츠를 제작하고, 메타버스와 같은 새로운 플랫폼을 이용해 가상 공연을 개최하는 등의 혁신적인 접근법을 시도하고 있습니다.',\n",
              " '__index_level_0__': 0,\n",
              " 'formatted_text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n음악산업의 디지털화가 진행됨에 따라, 전통적인 음반 중심의 수익 모델에서 디지털 음악 서비스 중심으로 변화했음을 설명하는데, 이러한 변화가 이루어진 주된 원인은 무엇이며, 이에 따라 업계가 어떻게 대응하고 있는지 구체적인 예를 들어 설명하시오.\\n\\n### Response:\\n음악산업의 디지털화 주된 원인은 인터넷과 мобиль 서비스의 발달로 인해 소비자들이 음악을 접근하고 소비하는 방식이 변화했기 때문입니다. 디지털 플랫폼의 등장으로 음원 스트리밍 서비스(예: 멜론, 스포티파이 등)가 보편화되면서, 소비자들은 과거의 음반 구매 대신 구독 기반의 서비스를 통해 음악에 쉽게 접근하게 되었습니다.\\n\\n이에 따라 음악업계는 다양한 대응 전략을 마련하고 있습니다. 예를 들어, 아티스트들은 음원 스트리밍 수익을 최적화하기 위해 SNS와 유튜브를 활용한 마케팅을 강화하고 있으며, 라이브 공연 및 팬미팅과 같은 오프라인 이벤트를 확대하여 추가 수익을 창출하고 있습니다. 또한, 일부 음악 레이블은 아티스트와의 협업을 통해 독점 콘텐츠를 제작하고, 메타버스와 같은 새로운 플랫폼을 이용해 가상 공연을 개최하는 등의 혁신적인 접근법을 시도하고 있습니다.<|im_end|>'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNT3z9nNVtZH"
      },
      "source": [
        "## 4. 모델 학습\n",
        "\n",
        "full train을 위해서는 `num_train_epochs=1`로 설정하고, `max_steps`를 설정하지 않으면 됩니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n7K5p11fVtZH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"formatted_text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # True로 설정하면 짧은 텍스트 데이터에 대해서는 더 빠른 학습 속도로를 보여줍니다.\n",
        "    args = TrainingArguments( # TrainingArguments는 자신의 학습 환경과 기호에 따라 적절하게 설정하면 됩니다.\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 42,\n",
        "        output_dir = \"./outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "97c2KpwfbNmP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 25,951 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 32 | Total steps = 811\n",
            " \"-____-\"     Number of trainable parameters = 161,480,704\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers, TRL and Unsloth!\n",
            "`pip install --upgrade --no-cache-dir unsloth git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git`\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='85' max='811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 85/811 25:25 < 3:42:23, 0.05 it/s, Epoch 0.10/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.241400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.110600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.090700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.033500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.065700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.015000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZl4bf7ZVtZI"
      },
      "source": [
        "## 5. 모델 추론"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHa2tCFgVtZI"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt_format.format(\n",
        "        \"수요와 공급의 원리에 대해서 설명해줘.\", # instruction\n",
        "        \"\", # output 생성을 위해 빈 칸으로 설정\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ9S7MWeVtZI"
      },
      "source": [
        "## 6. 모델 저장 및 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DsyXPKqVtZI"
      },
      "outputs": [],
      "source": [
        "# LoRA Adapter 저장\n",
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "\n",
        "# Merged model 저장 및 업로드\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(\"KR-X-AI/krx-qwen2-7b-instruct-v0\", tokenizer, save_method = \"merged_16bit\", token = \"\") # 개인 huggingface token을 사용하여 업로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAJJR1YVLYdg"
      },
      "source": [
        "## 참고자료\n",
        "\n",
        "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
        "- [Unsloth Docs](https://docs.unsloth.ai/)\n",
        "- [Unsloth Meta-Llama-3.1-8B Finetuning Tutorial](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)\n",
        "- [Huggingface PEFT](https://github.com/huggingface/peft)\n",
        "- [Huggingface TRL](https://github.com/huggingface/trl)\n",
        "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
        "- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
